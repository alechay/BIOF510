{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing a basic ANN in Pytorch\n",
    "\n",
    "This tutorial will cover everything you need to train and test a basic ANN in pytorch. Before we get into that, we will redo all the things we did in the data preparation and ANN class tutorial as a reminder!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting into the neural network\n",
    "Remember our BIOF510 class from last time? Now we are going to finish it so it can take some data and labels, create a neural network, train it, and test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIOF510:\n",
    "    \n",
    "    '''\n",
    "        \n",
    "        Inside this Net class, we can define what we want our neural network to look like!\n",
    "        We will define the layers and the sizes of the layers here - this is just a basic ANN \n",
    "        \n",
    "        Inputs:\n",
    "        \n",
    "        n_features - how many features does one sample of our data have (how many columns does the matrix have)\n",
    "        \n",
    "        hidden_dimension - the number of hidden neurons we want?\n",
    "        \n",
    "        n_classes - the number of unique labels in our data (i.e. 0,1 for the Breast Cancer dataset)\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    class Net(nn.Module):\n",
    "        \n",
    "        def __init__(self,n_features,hidden_dimension,n_classes):\n",
    "        \n",
    "            ##### calling the constructor of the parent class - gets everything we need from Pytorch\n",
    "            super(BIOF510.Net, self).__init__()\n",
    "            \n",
    "            ''' When dealing with nn.Linear, the first input is the size of the input data,\n",
    "            and the second input is how big you want the next layer to be '''\n",
    "            \n",
    "            ### The data enters here, then we make the next layer (hidden neurons)\n",
    "            self.input_layer = nn.Linear(n_features,hidden_dimension)\n",
    "            \n",
    "            ### hidden layer #1\n",
    "            self.layer1 = nn.Linear(hidden_dimension,hidden_dimension)\n",
    "            \n",
    "            ### hidden layer #2\n",
    "            self.layer2 = nn.Linear(hidden_dimension, hidden_dimension)\n",
    "            \n",
    "            ### The output layer, where we end up with a series of nodes corresponding to each of our uniquelabels\n",
    "            self.output_layer = nn.Linear(hidden_dimension,n_classes)\n",
    "            \n",
    "            '''\n",
    "              After each layer, we will apply nn.ReLU to transform our data into a nonlinear space\n",
    "              We input our data into this function, and ReLU is applied!\n",
    "            \n",
    "            '''\n",
    "            self.relu = nn.ReLU()\n",
    "       \n",
    "    \n",
    "    \n",
    "        '''\n",
    "        Now, we have to define the forward method, which takes a data point, or, in most cases, a batch, and\n",
    "        feeds it through all the layers of our neural network until assigning it a layer\n",
    "        \n",
    "        nn.Linear takes one array as an input, so we will input our data right into each layer, and then input the\n",
    "        outputs of each layer into the next layer\n",
    "        \n",
    "        After each layer, we will apply nn.ReLU to transform our data into a nonlinear space\n",
    "        \n",
    "        Finally, after the data has been passed through the output layer, we will convert it into a probaboility\n",
    "        distribution using the softmax function. \n",
    "        \n",
    "        This probabilty dsistribution will be used to assign a label to our\n",
    "        data points and to figure out just how well our neural network did, as we learned earlier today\n",
    "        \n",
    "        '''\n",
    "        def forward(self,batch):\n",
    "            \n",
    "            ## put the data into the input layer of the neural network\n",
    "            batch = self.input_layer(batch)\n",
    "            \n",
    "            batch = self.relu(batch)\n",
    "      \n",
    "            ## put the transformed data into the first hidden layer of the neural network\n",
    "            batch = self.layer1(batch)\n",
    "            \n",
    "            ## apply the ReLU function to the output of the 1st hidden layer\n",
    "            batch = self.relu(batch)\n",
    "            \n",
    "            ## put the transformed data into the second hidden layer of the neural network\n",
    "            batch = self.layer2(batch)\n",
    "            \n",
    "            ## apply the ReLU function to the output of the 1st hidden layer\n",
    "            batch = self.relu(batch)\n",
    "            \n",
    "            ## put the transformed data into the output layer of the neural network\n",
    "            batch = self.output_layer(batch)\n",
    "            \n",
    "            ### return the probability distribution via the softmax function\n",
    "            return nn.functional.softmax(batch)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Constructor\n",
    " \n",
    "We will add a constructor to our BIOF 510 class that takes a dataset and labels and makes them attributes of the class. We will define the constructor here before adding it into our class.\n",
    "\n",
    "### Labels for pytorch must be numbers starting at zero and increasing sequentially (i.e. [0,1,2]) so always make sure that is the case! Otherwise, make sure it the case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIOF510:\n",
    "    \n",
    "        \n",
    "    \n",
    "    '''\n",
    "        \n",
    "        Inside this BIOF510 class, we can input our data and labels\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,data,labels,tst_size=0.2,n_epochs=4):\n",
    "        \n",
    "        self.data = data\n",
    "        self.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we will add this constructor into our class-in-progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIOF510:\n",
    "\n",
    "        \n",
    "    '''\n",
    "        \n",
    "        Inside this BIOF510 class, we can input our data and labels\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,data,labels,tst_size=0.2,n_epochs=4):\n",
    "        \n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "      \n",
    "    \n",
    "    '''\n",
    "        \n",
    "        Inside this Net class, we can define what we want our neural network to look like!\n",
    "        We will define the layers and the sizes of the layers here - this is just a basic ANN \n",
    "        \n",
    "        Inputs:\n",
    "        \n",
    "        n_features - how many features does one sample of our data have (how many columns does the matrix have)\n",
    "        \n",
    "        hidden_dimension - the number of hidden neurons we want\n",
    "        \n",
    "        n_classes - the number of unique labels in our data (i.e. 0,1 for the Breast Cancer dataset)\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    class Net(nn.Module):\n",
    "        \n",
    "        def __init__(self,n_features,hidden_dimension,n_classes):\n",
    "        \n",
    "            ##### calling the constructor of the parent class - gets everything we need from Pytorch\n",
    "            super(BIOF510.Net, self).__init__()\n",
    "            \n",
    "            ''' When dealing with nn.Linear, the first input is the size of the input data,\n",
    "            and the second input is how big you want the next layer to be '''\n",
    "            \n",
    "            ### The data enters here, then we make the next layer (hidden neurons)\n",
    "            self.input_layer = nn.Linear(n_features,hidden_dimension)\n",
    "            \n",
    "            ### hidden layer #1\n",
    "            self.layer1 = nn.Linear(hidden_dimension,hidden_dimension)\n",
    "            \n",
    "            ### hidden layer #2\n",
    "            self.layer2 = nn.Linear(hidden_dimension, hidden_dimension)\n",
    "            \n",
    "            ### The output layer, where we end up with a series of nodes corresponding to each of our uniquelabels\n",
    "            self.output_layer = nn.Linear(hidden_dimension,n_classes)\n",
    "            \n",
    "            '''\n",
    "              After each layer, we will apply nn.ReLU to transform our data into a nonlinear space\n",
    "              We input our data into this function, and ReLU is applied!\n",
    "            \n",
    "            '''\n",
    "            self.relu = nn.ReLU()\n",
    "       \n",
    "    \n",
    "    \n",
    "        '''\n",
    "        Now, we have to define the forward method, which takes a data point, or, in most cases, a batch, and\n",
    "        feeds it through all the layers of our neural network until assigning it a layer\n",
    "        \n",
    "        nn.Linear takes one array as an input, so we will input our data right into each layer, and then input the\n",
    "        outputs of each layer into the next layer\n",
    "   \n",
    "        Finally, after the data has been passed through the output layer, we will convert it into a probaboility\n",
    "        distribution using the softmax function. \n",
    "        \n",
    "        This probabilty dsistribution will be used to assign a label to our\n",
    "        data points and to figure out just how well our neural network did, as we learned earlier today\n",
    "        \n",
    "        '''\n",
    "        def forward(self,batch):\n",
    "            \n",
    "            ## put the data into the input layer of the neural network\n",
    "            batch = self.input_layer(batch)\n",
    "            \n",
    "            batch = self.relu(batch)\n",
    "      \n",
    "            ## put the transformed data into the first hidden layer of the neural network\n",
    "            batch = self.layer1(batch)\n",
    "            \n",
    "            ## apply the ReLU function to the output of the 1st hidden layer\n",
    "            batch = self.relu(batch)\n",
    "            \n",
    "            ## put the transformed data into the second hidden layer of the neural network\n",
    "            batch = self.layer2(batch)\n",
    "            \n",
    "            ## apply the ReLU function to the output of the 1st hidden layer\n",
    "            batch = self.relu(batch)\n",
    "            \n",
    "            ## put the transformed data into the output layer of the neural network\n",
    "            batch = self.output_layer(batch)\n",
    "            \n",
    "            ### return the probability distribution via the softmax function\n",
    "            return nn.functional.softmax(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding in our batchify function from last time\n",
    "We know we will need to batch our dataset, so we will add in our \"batchify\" function from last time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIOF510:\n",
    "\n",
    "        \n",
    "    '''\n",
    "        \n",
    "        Inside this BIOF510 class, we can input our data and labels\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,data,labels,tst_size=0.2,n_epochs=4):\n",
    "        \n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "      \n",
    "    \n",
    "    '''\n",
    "        \n",
    "        Inside this Net class, we can define what we want our neural network to look like!\n",
    "        We will define the layers and the sizes of the layers here - this is just a basic ANN \n",
    "        \n",
    "        Inputs:\n",
    "        \n",
    "        n_features - how many features does one sample of our data have (how many columns does the matrix have)\n",
    "        \n",
    "        hidden_dimension - the number of hidden neurons we want?\n",
    "        \n",
    "        n_classes - the number of unique labels in our data (i.e. 0,1 for the Breast Cancer dataset)\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    class Net(nn.Module):\n",
    "        \n",
    "        def __init__(self,n_features,hidden_dimension,n_classes):\n",
    "        \n",
    "            ##### calling the constructor of the parent class - gets everything we need from Pytorch\n",
    "            super(BIOF510.Net, self).__init__()\n",
    "            \n",
    "            ''' When dealing with nn.Linear, the first input is the size of the input data,\n",
    "            and the second input is how big you want the next layer to be '''\n",
    "            \n",
    "            ### The data enters here, then we make the next layer (hidden neurons)\n",
    "            self.input_layer = nn.Linear(n_features,hidden_dimension)\n",
    "            \n",
    "            ### hidden layer #1\n",
    "            self.layer1 = nn.Linear(hidden_dimension,hidden_dimension)\n",
    "            \n",
    "            ### hidden layer #2\n",
    "            self.layer2 = nn.Linear(hidden_dimension, hidden_dimension)\n",
    "            \n",
    "            ### The output layer, where we end up with a series of nodes corresponding to each of our uniquelabels\n",
    "            self.output_layer = nn.Linear(hidden_dimension,n_classes)\n",
    "            \n",
    "            '''\n",
    "              After each layer, we will apply nn.ReLU to transform our data into a nonlinear space\n",
    "              We input our data into this function, and ReLU is applied!\n",
    "            \n",
    "            '''\n",
    "            self.relu = nn.ReLU()\n",
    "       \n",
    "    \n",
    "    \n",
    "        '''\n",
    "        Now, we have to define the forward method, which takes a data point, or, in most cases, a batch, and\n",
    "        feeds it through all the layers of our neural network until assigning it a layer\n",
    "        \n",
    "        nn.Linear takes one array as an input, so we will input our data right into each layer, and then input the\n",
    "        outputs of each layer into the next layer\n",
    "        \n",
    "        Finally, after the data has been passed through the output layer, we will convert it into a probaboility\n",
    "        distribution using the softmax function. \n",
    "        \n",
    "        This probabilty dsistribution will be used to assign a label to our\n",
    "        data points and to figure out just how well our neural network did, as we learned earlier today\n",
    "        \n",
    "        '''\n",
    "        def forward(self,batch):\n",
    "            \n",
    "            ## put the data into the input layer of the neural network\n",
    "            batch = self.input_layer(batch)\n",
    "            \n",
    "            batch = self.relu(batch)\n",
    "      \n",
    "            ## put the transformed data into the first hidden layer of the neural network\n",
    "            batch = self.layer1(batch)\n",
    "            \n",
    "            ## apply the ReLU function to the output of the 1st hidden layer\n",
    "            batch = self.relu(batch)\n",
    "            \n",
    "            ## put the transformed data into the second hidden layer of the neural network\n",
    "            batch = self.layer2(batch)\n",
    "            \n",
    "            ## apply the ReLU function to the output of the 1st hidden layer\n",
    "            batch = self.relu(batch)\n",
    "            \n",
    "            ## put the transformed data into the output layer of the neural network\n",
    "            batch = self.output_layer(batch)\n",
    "            \n",
    "            ### return the probability distribution via the softmax function\n",
    "            return nn.functional.softmax(batch)\n",
    "\n",
    "        \n",
    "        \n",
    "''' Utility Function - function to turn the data into batches'''\n",
    "\n",
    "def batchify(data,labels,batch_size=16):\n",
    "    \n",
    "    batches= []\n",
    "    label_batches = []\n",
    "\n",
    "\n",
    "    for n in range(0,len(data),batch_size):\n",
    "        if n+batch_size < len(data):\n",
    "            batches.append(data[n:n+batch_size])\n",
    "            label_batches.append(labels[n:n+batch_size])\n",
    "\n",
    "    if len(data)%batch_size > 0:\n",
    "        batches.append(data[len(data)-(len(data)%batch_size):len(data)])\n",
    "        label_batches.append(labels[len(data)-(len(data)%batch_size):len(data)])\n",
    "        \n",
    "    return batches,label_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the neural network\n",
    "Now, we need to define a function that splits and batchifies the data, then creates and trains a neural network\n",
    "\n",
    "So we can easily customize the analysis, our train function will take as input parameters:\n",
    "1. the size of the testing set we want\n",
    "2. The number of epochs we want (how many times we move through the dataset)\n",
    "3. the size of the hidden layers we want (how many hidden neurons)\n",
    "4. The size of our batches\n",
    "\n",
    "## Splitting:\n",
    "In most cases, datasets that need NNs are too large and NNs take too long to do cross-validation, so we will just split the data one time using sklearn's train_test_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "        \n",
    "        We can specify the size of the test dataset, and the\n",
    "        number of epochs (number of times we move through the data), the size of the hidden layers, and the \n",
    "        size of the batches here in the train_test function for easy customizations\n",
    "        \n",
    "        Inside the function, our data will be broken down into train and test sets (arrays)\n",
    "        using sklearn's train_test_split function.\n",
    "        \n",
    "        From there, it will be batchified using our batchify function from the preparation tutorial\n",
    "        \n",
    "        Finally, a neural network will be generated and trained on our dataset!\n",
    "        \n",
    "'''\n",
    "\n",
    "def train_test(self,test_size,n_epochs,hidden_dimensions,batch_size,lr):\n",
    "            \n",
    "        ### splitting the data into a training/testing set\n",
    "        train_data,test_data,train_labels,test_labels = train_test_split(self.data,self.labels, test_size=test_size)\n",
    "        \n",
    "        ## creating the batches using the batchify function\n",
    "        train_batches,train_label_batches = batchify(train_data,train_labels,batch_size=batch_size)\n",
    "        \n",
    "        '''\n",
    "        Here is where we define our neural network model - the Net class is inside BIOF510, so we have to call\n",
    "        it accordingly \n",
    "        \n",
    "        We use the length of our first data point to set the length of our input data (they are all the same)\n",
    "        \n",
    "        The number of class is equal to the number of unique values (the set) of our training labels\n",
    "        '''\n",
    "        neural_network = BIOF510_Final.Net(len(train_data[0]),hidden_dimensions,len(set(train_labels)))\n",
    "        \n",
    "         \n",
    "    \n",
    "        '''\n",
    "        The train function tells the neural network that it is about to be trained and that it \n",
    "        will have to calculate the needed information for optimization \n",
    "        \n",
    "        This function should always be called before training\n",
    "        '''\n",
    "        neural_network.train()\n",
    "        \n",
    "        \n",
    "        ''' This loop moves through the data once for each epoch'''\n",
    "        for i in range(n_epochs):\n",
    "            \n",
    "            ### track the number we get correct\n",
    "            correct = 0\n",
    "            \n",
    "            ''' This loop moves through each batch and feeds them into the neural network'''\n",
    "            for ii in range(len(train_batches)):\n",
    "                \n",
    "                batch = train_batches[ii]\n",
    "                labels = train_label_batches[ii]\n",
    "\n",
    "            \n",
    "                ''' \n",
    "                Puts our batch into the neural network\n",
    "\n",
    "                Predictions: For each data point in our batch, we would get something that looks like:\n",
    "                tensor([0.3,0.7]) where each number corresponds to the probability of a class\n",
    "                '''\n",
    "                \n",
    "                predictions = neural_network(torch.tensor(batch.astype(np.float32)))\n",
    "                \n",
    "        return neural_network\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE TRAIN FUNCTION WILL NOT WORK\n",
    "Why? Because the data type is all wrong. The pytorch data type is called a tensor (kind of like an array), and Pytorch neural networks require tensors as inputs. We must convert our data (either a list or an array) to a tensor with the torch.tensor function! If your data is not a list or an array, you need to make it into one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    " def train_test(self,test_size,n_epochs,hidden_dimensions,batch_size,lr):\n",
    "            \n",
    "        ### splitting the data into a training/testing set\n",
    "        train_data,test_data,train_labels,test_labels = train_test_split(self.data,self.labels, test_size=test_size)\n",
    "        \n",
    "        ## creating the batches using the batchify function\n",
    "        train_batches,train_label_batches = batchify(train_data,train_labels,batch_size=batch_size)\n",
    "        \n",
    "        '''\n",
    "        Here is where we define our neural network model - the Net class is inside BIOF510, so we have to call\n",
    "        it accordingly \n",
    "        \n",
    "        We use the length of our first data point to set the length of our input data (they are all the same)\n",
    "        \n",
    "        The number of class is equal to the number of unique values (the set) of our training labels\n",
    "        '''\n",
    "        neural_network = BIOF510_Final.Net(len(train_data[0]),hidden_dimensions,len(set(train_labels)))\n",
    "              \n",
    "        \n",
    "        '''\n",
    "        The train function tells the neural network that it is about to be trained and that it \n",
    "        will have to calculate the needed information for optimization \n",
    "        \n",
    "        This function should always be called before training\n",
    "        '''\n",
    "        neural_network.train()\n",
    "        \n",
    "        \n",
    "        ''' This loop moves through the data once for each epoch'''\n",
    "        for i in range(n_epochs):\n",
    "            \n",
    "            ### track the number we get correct\n",
    "            correct = 0\n",
    "            \n",
    "            ''' This loop moves through each batch and feeds into the neural network'''\n",
    "            for ii in range(len(train_batches)):\n",
    "                \n",
    "                batch = train_batches[ii]\n",
    "                labels = train_label_batches[ii]\n",
    "\n",
    "                \n",
    "                ''' \n",
    "                Puts our batch into the neural network after converting it to a tensor\n",
    "                \n",
    "                Pytorch wants numeric data to be floats, so we will convert to a float as well \n",
    "                using np.float32\n",
    "                \n",
    "                Predictions: For each data point in our batch, we would get something that looks like:\n",
    "                tensor([0.3,0.7]) where each number corresponds to the probability of a class\n",
    "                \n",
    "                \n",
    "                '''\n",
    "                predictions = neural_network(torch.tensor(batch.astype(np.float32)))\n",
    "                \n",
    "        return neural_network\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE TRAIN FUNCTION WILL NOT LEARN ANYTHING\n",
    "Why? Because there is no optimization function or loss function!\n",
    "\n",
    "1. We will first define a stochastic gradient descent otpimization function using the torch.optim package\n",
    "    We must choose a learning rate for this function, so that will be a parameter of our train_test function (lr)\n",
    "    \n",
    "2. We will then define a cross-entropy loss function through the nn module\n",
    "\n",
    "3. We will then implement both these functions in our training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    " def train_test(self,test_size,n_epochs,hidden_dimensions,batch_size,lr):\n",
    "            \n",
    "        ### splitting the data into a training/testing set\n",
    "        train_data,test_data,train_labels,test_labels = train_test_split(self.data,self.labels, test_size=test_size)\n",
    "        \n",
    "        ## creating the batches using the batchify function\n",
    "        train_batches,train_label_batches = batchify(train_data,train_labels,batch_size=batch_size)\n",
    "        \n",
    "        '''\n",
    "        Here is where we define our neural network model - the Net class is inside BIOF510, so we have to call\n",
    "        it accordingly \n",
    "        \n",
    "        We use the length of our first data point to set the length of our input data (they are all the same)\n",
    "        \n",
    "        The number of class is equal to the number of unique values (the set) of our training labels\n",
    "        '''\n",
    "        neural_network = BIOF510_Final.Net(len(train_data[0]),hidden_dimensions,len(set(train_labels)))\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Here, we use the torch.optim package to create our stochastic gradient descent function\n",
    "        \n",
    "        neural_network.parameters() reads internal information from our NN \n",
    "        (don't worry about that - SGD just requires it)\n",
    "        \n",
    "        lr is the learning rate\n",
    "        '''\n",
    "        optimizer = optim.SGD(neural_network.parameters(), lr=lr)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Here, we use the nn package to create our cross entropy loss function\n",
    "        '''\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        \n",
    "                \n",
    "        '''\n",
    "        The train function tells the neural network that it is about to be trained and that it \n",
    "        will have to calculate the needed information for optimization \n",
    "        \n",
    "        This function should always be called before training\n",
    "        '''\n",
    "        neural_network.train()\n",
    "        \n",
    "        \n",
    "        ''' This loop moves through the data once for each epoch'''\n",
    "        for i in range(n_epochs):\n",
    "            \n",
    "            ### track the number we get correct\n",
    "            correct = 0\n",
    "            \n",
    "            ''' This loop moves through each batch and feeds into the neural network'''\n",
    "            for ii in range(len(train_batches)):\n",
    "                \n",
    "                ''' \n",
    "                Clears previous gradients from the optimizer - the optimizer,\n",
    "                in this case, does not need to know what happened last time\n",
    "                '''\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                \n",
    "                batch = train_batches[ii]\n",
    "                labels = train_label_batches[ii]\n",
    "\n",
    "                \n",
    "                ''' \n",
    "                Puts our batch into the neural network after converting it to a tensor\n",
    "                \n",
    "                Pytorch wants numeric data to be floats, so we will convert to a float as well \n",
    "                using np.float32\n",
    "                \n",
    "                Predictions: For each data point in our batch, we would get something that looks like:\n",
    "                tensor([0.3,0.7]) where each number corresponds to the probability of a class\n",
    "                \n",
    "                '''\n",
    "                predictions = neural_network(torch.tensor(batch.astype(np.float32)))\n",
    "                \n",
    "                \n",
    "                ''' \n",
    "                We put our probabilities into the loss function to calculate the error for this batch\n",
    "                \n",
    "                '''\n",
    "                loss = loss_function(predictions,torch.LongTensor(labels))\n",
    "                \n",
    "                '''\n",
    "                loss.backward calculates the partial derivatives that we need to optimize\n",
    "                '''\n",
    "                loss.backward()\n",
    "                \n",
    "                \n",
    "                '''\n",
    "                optimizer step calculates the weight updates so the neural network can update the weights \n",
    "                '''\n",
    "                optimizer.step()\n",
    "                \n",
    "           \n",
    "        return neural_network\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Neural Network\n",
    "We now have a function to train our dataset, but how do we know if we overfit or not? Our neural network \n",
    "needs to learn things that can be applied to blind data, which makes it useful in areas like clinical decision making. After training the network, it needs to work on all the data we put into it from patients in the clinic!\n",
    "\n",
    "We will now update our train_test function to predict the labels of our test dataset using the weights we already optimized with our SGD function (they will not be updated any further). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    " def train_test(self,test_size,n_epochs,hidden_dimensions,batch_size,lr):\n",
    "            \n",
    "        ### splitting the data into a training/testing set\n",
    "        train_data,test_data,train_labels,test_labels = train_test_split(self.data,self.labels, test_size=test_size)\n",
    "        \n",
    "        ## creating the batches using the batchify function\n",
    "        train_batches,train_label_batches = batchify(train_data,train_labels,batch_size=batch_size)\n",
    "        \n",
    "        '''\n",
    "        Here is where we define our neural network model - the Net class is inside BIOF510, so we have to call\n",
    "        it accordingly \n",
    "        \n",
    "        We use the length of our first data point to set the length of our input data (they are all the same)\n",
    "        \n",
    "        The number of class is equal to the number of unique values (the set) of our training labels\n",
    "        '''\n",
    "        neural_network = BIOF510_Final.Net(len(train_data[0]),hidden_dimensions,len(set(train_labels)))\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Here, we use the torch.optim package to create our stochastic gradient descent function\n",
    "        \n",
    "        neural_network.parameters() reads internal information from our NN \n",
    "        (don't worry about that - SGD just requires it)\n",
    "        \n",
    "        lr is the learning rate\n",
    "        '''\n",
    "        optimizer = optim.SGD(neural_network.parameters(), lr=lr)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Here, we use the nn package to create our cross entropy loss function\n",
    "        '''\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        \n",
    "                \n",
    "        '''\n",
    "        The train function tells the neural network that it is about to be trained and that it \n",
    "        will have to calculate the needed information for optimization \n",
    "        \n",
    "        This function should always be called before training\n",
    "        '''\n",
    "        neural_network.train()\n",
    "        \n",
    "        \n",
    "        ''' This loop moves through the data once for each epoch'''\n",
    "        for i in range(n_epochs):\n",
    "            \n",
    "            ### track the number we get correct\n",
    "            correct = 0\n",
    "            \n",
    "            ''' This loop moves through each batch and feeds into the neural network'''\n",
    "            for ii in range(len(train_batches)):\n",
    "                \n",
    "                ''' \n",
    "                Clears previous gradients from the optimizer - the optimizer,\n",
    "                in this case, does not need to know what happened last time\n",
    "                '''\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                \n",
    "                batch = train_batches[ii]\n",
    "                labels = train_label_batches[ii]\n",
    "\n",
    "                \n",
    "                ''' \n",
    "                Puts our batch into the neural network after converting it to a tensor\n",
    "                \n",
    "                Pytorch wants numeric data to be floats, so we will convert to a float as well \n",
    "                using np.float32\n",
    "                \n",
    "                Predictions: For each data point in our batch, we would get something that looks like:\n",
    "                tensor([0.3,0.7]) where each number corresponds to the probability of a class\n",
    "                '''\n",
    "                predictions = neural_network(torch.tensor(batch.astype(np.float32)))\n",
    "                \n",
    "                \n",
    "                ''' \n",
    "                We put our probabilities into the loss function to calculate the error for this batch\n",
    "                \n",
    "                '''\n",
    "                loss = loss_function(predictions,torch.LongTensor(labels))\n",
    "                \n",
    "                '''\n",
    "                loss.backward calculates the partial derivatives that we need to optimize\n",
    "                '''\n",
    "                loss.backward()\n",
    "                \n",
    "                \n",
    "                '''\n",
    "                optimizer step calculates the weight updates so the neural network can update the weights \n",
    "                '''\n",
    "                optimizer.step()\n",
    "                       \n",
    "        '''\n",
    "        The eval function tells the neural network that it is about to be tested on blind test data\n",
    "        and shouldn't change any of its internal parameters\n",
    "        \n",
    "        This function should always be called before eval\n",
    "        '''\n",
    "        neural_network.eval()\n",
    "        \n",
    "        test_correct = 0\n",
    "        \n",
    "        ''' input our test data into the neural network'''\n",
    "        predictions = neural_network(torch.tensor(test_data.astype(np.float32)))\n",
    "\n",
    "        return neural_network\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But how will we know? \n",
    "We need something in our training function that prints out the train and testing accuracies! So we can track our analysis as it runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def train_test(self,test_size,n_epochs,hidden_dimensions,batch_size,lr):\n",
    "            \n",
    "        ### splitting the data into a training/testing set\n",
    "        train_data,test_data,train_labels,test_labels = train_test_split(self.data,self.labels, test_size=test_size)\n",
    "        \n",
    "        ## creating the batches using the batchify function\n",
    "        train_batches,train_label_batches = batchify(train_data,train_labels,batch_size=batch_size)\n",
    "        \n",
    "        '''\n",
    "        Here is where we define our neural network model - the Net class is inside BIOF510, so we have to call\n",
    "        it accordingly \n",
    "        \n",
    "        We use the length of our first data point to set the length of our input data (they are all the same)\n",
    "        \n",
    "        The number of class is equal to the number of unique values (the set) of our training labels\n",
    "        '''\n",
    "        neural_network = BIOF510_Final.Net(len(train_data[0]),hidden_dimensions,len(set(train_labels)))\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Here, we use the torch.optim package to create our stochastic gradient descent function\n",
    "        \n",
    "        neural_network.parameters() reads internal information from our NN \n",
    "        (don't worry about that - SGD just requires it)\n",
    "        \n",
    "        lr is the learning rate\n",
    "        '''\n",
    "        optimizer = optim.SGD(neural_network.parameters(), lr=lr)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Here, we use the nn package to create our cross entropy loss function\n",
    "        '''\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        \n",
    "                \n",
    "        '''\n",
    "        The train function tells the neural network that it is about to be trained and that it \n",
    "        will have to calculate the needed information for optimization \n",
    "        \n",
    "        This function should always be called before training\n",
    "        '''\n",
    "        neural_network.train()\n",
    "        \n",
    "        \n",
    "        ''' This loop moves through the data once for each epoch'''\n",
    "        for i in range(n_epochs):\n",
    "            \n",
    "            ### track the number we get correct\n",
    "            correct = 0\n",
    "            \n",
    "            ''' This loop moves through each batch and feeds into the neural network'''\n",
    "            for ii in range(len(train_batches)):\n",
    "                \n",
    "                ''' \n",
    "                Clears previous gradients from the optimizer - the optimizer,\n",
    "                in this case, does not need to know what happened last time\n",
    "                '''\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                \n",
    "                batch = train_batches[ii]\n",
    "                labels = train_label_batches[ii]\n",
    "\n",
    "                \n",
    "                ''' \n",
    "                Puts our batch into the neural network after converting it to a tensor\n",
    "                \n",
    "                Pytorch wants numeric data to be floats, so we will convert to a float as well \n",
    "                using np.float32\n",
    "                \n",
    "                Predictions: For each data point in our batch, we would get something that looks like:\n",
    "                tensor([0.3,0.7]) where each number corresponds to the probability of a class\n",
    "                '''\n",
    "                predictions = neural_network(torch.tensor(batch.astype(np.float32)))\n",
    "                \n",
    "                \n",
    "                ''' \n",
    "                We put our probabilities into the loss function to calculate the error for this batch\n",
    "                \n",
    "                '''\n",
    "                loss = loss_function(predictions,torch.LongTensor(labels))\n",
    "                \n",
    "                '''\n",
    "                loss.backward calculates the partial derivatives that we need to optimize\n",
    "                '''\n",
    "                loss.backward()\n",
    "                \n",
    "                \n",
    "                '''\n",
    "                optimizer step calculates the weight updates so the neural network can update the weights \n",
    "                '''\n",
    "                optimizer.step()\n",
    "                \n",
    "                \n",
    "                '''\n",
    "                We extract just the data from our predictions, not other stuff Pytorch includes in that object\n",
    "                \n",
    "                We can then use the argmax function to figure out which index corresponds to the highest probability.\n",
    "                If it is the 0th index, and the label is zero, we add one to correct. \n",
    "                If it is the 1st index, and the label is one, we add one to correct.\n",
    "                \n",
    "                This is why the labels need to start at zero and increase sequentially!\n",
    "                \n",
    "                '''\n",
    "                for n,pred in enumerate(predictions.data):\n",
    "                    if labels[n] == torch.argmax(pred):\n",
    "                        correct += 1\n",
    "                        \n",
    "                        \n",
    "            print(\"Accuracy for Epoch # \" + str(i) + \": \" + str(correct/len(train_data)))\n",
    "\n",
    "        print()\n",
    "        \n",
    "\n",
    "                    \n",
    "        '''\n",
    "        The eval function tells the neural network that it is about to be tested on blind test data\n",
    "        and shouldn't change any of its internal parameters\n",
    "        \n",
    "        This function should always be called before eval\n",
    "        '''\n",
    "        neural_network.eval()\n",
    "        \n",
    "        test_correct = 0\n",
    "        \n",
    "        ''' input our test data into the neural network'''\n",
    "        predictions = neural_network(torch.tensor(test_data.astype(np.float32)))\n",
    "        \n",
    "        ''' checks how many we got right - very simple!'''\n",
    "        for n,pred in enumerate(predictions.data):\n",
    "            if test_labels[n] == torch.argmax(pred):\n",
    "                    test_correct += 1\n",
    "                    \n",
    "        print(\"Accuracy on test set: \" + str(test_correct/len(test_data)))\n",
    "           \n",
    "        return neural_network\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final updates\n",
    "Now, we will add our train_test function to the BIOF510 class, which will now be called the BIOF510_Final class (used in Assignment #1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIOF510_Final:\n",
    "\n",
    "        \n",
    "    '''\n",
    "        \n",
    "        Inside this BIOF510 class, we can input our data and labels\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,data,labels,tst_size=0.2,n_epochs=4):\n",
    "        \n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "      \n",
    "    \n",
    "    '''\n",
    "        \n",
    "        Inside this Net class, we can define what we want our neural network to look like!\n",
    "        We will define the layers and the sizes of the layers here - this is just a basic ANN \n",
    "        \n",
    "        Inputs:\n",
    "        \n",
    "        n_features - how many features does one sample of our data have (how many columns does the matrix have)\n",
    "        \n",
    "        hidden_dimension - the number of hidden neurons we want?\n",
    "        \n",
    "        n_classes - the number of unique labels in our data (i.e. 0,1 for the Breast Cancer dataset)\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    class Net(nn.Module):\n",
    "        \n",
    "        def __init__(self,n_features,hidden_dimension,n_classes):\n",
    "        \n",
    "            ##### calling the constructor of the parent class - gets everything we need from Pytorch\n",
    "            super(BIOF510_Final.Net, self).__init__()\n",
    "            \n",
    "            ''' When dealing with nn.Linear, the first input is the size of the input data,\n",
    "            and the second input is how big you want the next layer to be '''\n",
    "            \n",
    "            ### The data enters here, then we make the next layer (hidden neurons)\n",
    "            self.input_layer = nn.Linear(n_features,hidden_dimension)\n",
    "            \n",
    "            ### hidden layer #1\n",
    "            self.layer1 = nn.Linear(hidden_dimension,hidden_dimension)\n",
    "            \n",
    "            ### hidden layer #2\n",
    "            self.layer2 = nn.Linear(hidden_dimension, hidden_dimension)\n",
    "            \n",
    "            ### The output layer, where we end up with a series of nodes corresponding to each of our uniquelabels\n",
    "            self.output_layer = nn.Linear(hidden_dimension,n_classes)\n",
    "            \n",
    "            '''\n",
    "              After each layer, we will apply nn.ReLU to transform our data into a nonlinear space\n",
    "              We input our data into this function, and ReLU is applied!\n",
    "            \n",
    "            '''\n",
    "            self.relu = nn.ReLU()\n",
    "       \n",
    "    \n",
    "    \n",
    "        '''\n",
    "        Now, we have to define the forward method, which takes a data point, or, in most cases, a batch, and\n",
    "        feeds it through all the layers of our neural network until assigning it a layer\n",
    "        \n",
    "        nn.Linear takes one array as an input, so we will input our data right into each layer, and then input the\n",
    "        outputs of each layer into the next layer\n",
    "        \n",
    "        After each layer, we will apply nn.ReLU to transform our data into a nonlinear space\n",
    "        \n",
    "        Finally, after the data has been passed through the output layer, we will convert it into a probaboility\n",
    "        distribution using the softmax function. \n",
    "        \n",
    "        This probabilty dsistribution will be used to assign a label to our\n",
    "        data points and to figure out just how well our neural network did, as we learned earlier today\n",
    "        \n",
    "        '''\n",
    "        def forward(self,batch):\n",
    "            \n",
    "            ## put the data into the input layer of the neural network\n",
    "            batch = self.input_layer(batch)\n",
    "            \n",
    "            batch = self.relu(batch)\n",
    "      \n",
    "            ## put the transformed data into the first hidden layer of the neural network\n",
    "            batch = self.layer1(batch)\n",
    "            \n",
    "            ## apply the ReLU function to the output of the 1st hidden layer\n",
    "            batch = self.relu(batch)\n",
    "            \n",
    "            ## put the transformed data into the second hidden layer of the neural network\n",
    "            batch = self.layer2(batch)\n",
    "            \n",
    "            ## apply the ReLU function to the output of the 1st hidden layer\n",
    "            batch = self.relu(batch)\n",
    "            \n",
    "            ## put the transformed data into the output layer of the neural network\n",
    "            batch = self.output_layer(batch)\n",
    "            print(batch)\n",
    "            \n",
    "            ### return the probability distribution via the softmax function\n",
    "            return nn.functional.softmax(batch)\n",
    "            \n",
    "    \n",
    "\n",
    "        \n",
    "    def train_test(self,test_size,n_epochs,hidden_dimensions,batch_size,lr):\n",
    "            \n",
    "        ### splitting the data into a training/testing set\n",
    "        train_data,test_data,train_labels,test_labels = train_test_split(self.data,self.labels, test_size=test_size)\n",
    "        \n",
    "        ## creating the batches using the batchify function\n",
    "        train_batches,train_label_batches = batchify(train_data,train_labels,batch_size=batch_size)\n",
    "        \n",
    "        '''\n",
    "        Here is where we define our neural network model - the Net class is inside BIOF510, so we have to call\n",
    "        it accordingly \n",
    "        \n",
    "        We use the length of our first data point to set the length of our input data (they are all the same)\n",
    "        \n",
    "        The number of class is equal to the number of unique values (the set) of our training labels\n",
    "        '''\n",
    "        neural_network = BIOF510_Final.Net(len(train_data[0]),hidden_dimensions,len(set(train_labels)))\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Here, we use the torch.optim package to create our stochastic gradient descent function\n",
    "        \n",
    "        neural_network.parameters() reads internal information from our NN \n",
    "        (don't worry about that - SGD just requires it)\n",
    "        \n",
    "        lr is the learning rate\n",
    "        '''\n",
    "        optimizer = optim.SGD(neural_network.parameters(), lr=lr)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Here, we use the nn package to create our cross entropy loss function\n",
    "        '''\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        \n",
    "                \n",
    "        '''\n",
    "        The train function tells the neural network that it is about to be trained and that it \n",
    "        will have to calculate the needed information for optimization \n",
    "        \n",
    "        This function should always be called before training\n",
    "        '''\n",
    "        neural_network.train()\n",
    "        \n",
    "        \n",
    "        ''' This loop moves through the data once for each epoch'''\n",
    "        for i in range(n_epochs):\n",
    "            \n",
    "            ### track the number we get correct\n",
    "            correct = 0\n",
    "            \n",
    "            ''' This loop moves through each batch and feeds into the neural network'''\n",
    "            for ii in range(len(train_batches)):\n",
    "                \n",
    "                ''' \n",
    "                Clears previous gradients from the optimizer - the optimizer,\n",
    "                in this case, does not need to know what happened last time\n",
    "                '''\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                \n",
    "                batch = train_batches[ii]\n",
    "                labels = train_label_batches[ii]\n",
    "\n",
    "                \n",
    "                ''' \n",
    "                Puts our batch into the neural network after converting it to a tensor\n",
    "                \n",
    "                Pytorch wants numeric data to be floats, so we will convert to a float as well \n",
    "                using np.float32\n",
    "                \n",
    "                Predictions: For each data point in our batch, we would get something that looks like:\n",
    "                tensor([0.3,0.7]) where each number corresponds to the probability of a class\n",
    "                '''\n",
    "                predictions = neural_network(torch.tensor(batch.astype(np.float32)))\n",
    "                \n",
    "                \n",
    "                ''' \n",
    "                We put our probabilities into the loss function to calculate the error for this batch\n",
    "                \n",
    "                '''\n",
    "                loss = loss_function(predictions,torch.LongTensor(labels))\n",
    "                \n",
    "                '''\n",
    "                loss.backward calculates the partial derivatives that we need to optimize\n",
    "                '''\n",
    "                loss.backward()\n",
    "                \n",
    "                \n",
    "                '''\n",
    "                optimizer step calculates the weight updates so the neural network can update the weights \n",
    "                '''\n",
    "                optimizer.step()\n",
    "                \n",
    "                \n",
    "                '''\n",
    "                We extract just the data from our predictions, not other stuff Pytorch includes in that object\n",
    "                \n",
    "                We can then use the argmax function to figure out which index corresponds to the highest probability.\n",
    "                If it is the 0th index, and the label is zero, we add one to correct. \n",
    "                If it is the 1st index, and the label is one, we add one to correct.\n",
    "                \n",
    "                This is why the labels need to start at zero and increase sequentially!\n",
    "                '''\n",
    "                for n,pred in enumerate(predictions.data):\n",
    "                    if labels[n] == torch.argmax(pred):\n",
    "                        correct += 1\n",
    "                        \n",
    "                        \n",
    "            print(\"Accuracy for Epoch # \" + str(i) + \": \" + str(correct/len(train_data)))\n",
    "\n",
    "        print()\n",
    "        \n",
    "\n",
    "                    \n",
    "        '''\n",
    "        The eval function tells the neural network that it is about to be tested on blind test data\n",
    "        and shouldn't change any of its internal parameters\n",
    "        \n",
    "        This function should always be called before eval\n",
    "        '''\n",
    "        neural_network.eval()\n",
    "        \n",
    "        test_correct = 0\n",
    "        \n",
    "        ''' input our test data into the neural network'''\n",
    "        predictions = neural_network(torch.tensor(test_data.astype(np.float32)))\n",
    "        \n",
    "        ''' this checks how many we got right - very simple!'''\n",
    "        for n,pred in enumerate(predictions.data):\n",
    "            if test_labels[n] == torch.argmax(pred):\n",
    "                    test_correct += 1\n",
    "                    \n",
    "        print(\"Accuracy on test set: \" + str(test_correct/len(test_data)))\n",
    "           \n",
    "        return neural_network\n",
    "        \n",
    "   \n",
    "\n",
    "\n",
    "''' Utility Function - function to turn the data into batches'''\n",
    "\n",
    "def batchify(data,labels,batch_size=16):\n",
    "    \n",
    "    batches= []\n",
    "    label_batches = []\n",
    "\n",
    "\n",
    "    for n in range(0,len(data),batch_size):\n",
    "        if n+batch_size < len(data):\n",
    "            batches.append(data[n:n+batch_size])\n",
    "            label_batches.append(labels[n:n+batch_size])\n",
    "\n",
    "    if len(data)%batch_size > 0:\n",
    "        batches.append(data[len(data)-(len(data)%batch_size):len(data)])\n",
    "        label_batches.append(labels[len(data)-(len(data)%batch_size):len(data)])\n",
    "        \n",
    "    return batches,label_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actually using our NN\n",
    "It's time to actually use our neural network, so we are going to use some real-life data. The dataset I provided (data.csv), needs to be in the same directory as this notebook. I recommend simply downloading them both and keeping them in downloads for simplicity. \n",
    "\n",
    "### Data.csv\n",
    "Data.csv contains protein expression values for 50,000 T-Cells. 25,000 come from patietns who did not respond to immunotherapy (label 1), and 25,000 cells come from patients who did respond to immunotherapy (label 0). We are going to use a neural network to see if we can predict which is which and determine if a patient should receive this immunotherapy treatment or not (precison medicine application!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-117882c4081d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#### load the data object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#### break it down into data and labels (for each index)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#### load the data object\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "\n",
    "#### break it down into data and labels (for each index)\n",
    "labels = data['score'].values\n",
    "data = data[[col for col in data.columns if col != 'score']].values\n",
    "\n",
    "print(data[0:5])\n",
    "print(labels[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MinMaxScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-97e538abb158>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mscaled_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MinMaxScaler' is not defined"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled_data = MinMaxScaler().fit_transform(data)\n",
    "\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play around with the parameters to see how different values change the training and testing accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BIOF510_Final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-73513c2d8754>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtestclass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBIOF510_Final\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_dimensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BIOF510_Final' is not defined"
     ]
    }
   ],
   "source": [
    "testclass = BIOF510_Final(scaled_data,labels)\n",
    "model = testclass.train_test(test_size=0.2,n_epochs=8,hidden_dimensions=100,batch_size=16,lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
