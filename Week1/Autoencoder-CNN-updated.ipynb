{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using our CNN\n",
    "Now, we will add in our train_test + batchify method from last time to use our CNN on a dataset! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIOF510_AC:\n",
    "    \n",
    "    \n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "\n",
    "    \n",
    "    \n",
    "    '''\n",
    "        \n",
    "        Inside this Net class, we can define what we want our convolutional neural network to look like!\n",
    "        We will define the convolutional layers AND the linear layers here \n",
    "        \n",
    "        Inputs:\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    class AutoCNN(nn.Module):\n",
    "        \n",
    "        def __init__(self):\n",
    "            \n",
    "            super(BIOF510_AC.AutoCNN, self).__init__()\n",
    "            ''' here, we define our convolution layers'''\n",
    "            self.conv1 = nn.Conv2d(1, 16, 3)\n",
    "            self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "            \n",
    "            ''' \n",
    "            max pooling - we need the indicies of the max vaues for unpooling\n",
    "            so return_indicies=True \n",
    "            '''\n",
    "            self.pool = nn.MaxPool2d(kernel_size=2,return_indices=True)\n",
    "            \n",
    "            ''' now need an unpool,to remove the effects of pooling in the decoder '''\n",
    "            self.unpool = nn.MaxUnpool2d(2)\n",
    "            self.tanh = nn.Tanh()\n",
    "            self.relu = nn.ReLU()\n",
    "            \n",
    "            ''' Our transpose covolution layer'''\n",
    "            self.unconv1 = nn.ConvTranspose2d(32, 16, 3)\n",
    "            self.unconv2 = nn.ConvTranspose2d(16, 1, 3)\n",
    "\n",
    "        def forward(self, x):\n",
    "            \n",
    "            x = self.conv1(x)\n",
    "            x = self.relu(x)\n",
    "            ''' pooling - indicies are returned '''\n",
    "            x,indices1 = self.pool(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.relu(x)\n",
    "            ''' pooling - indicies are returned'''\n",
    "            x,indices2 = self.pool(x)\n",
    "            \n",
    "            ''' how big is the unpooled image we want to recreate?\n",
    "            \n",
    "            well, we lose a pixel on each end when we do convolution because \n",
    "            the kernel has to fit onto the images, so the center pixel cannot be on the \n",
    "            edge unless we pad the images\n",
    "            \n",
    "            \n",
    "            We started with 28x28 -> 26x26 -> 13x13 (pooling) -> 11x11\n",
    "            \n",
    "            so we will need to unpool images of 11x11 and 26x26\n",
    "            '''\n",
    "            x = self.unpool(x,indices2,output_size=(11,11))\n",
    "            x = self.unconv1(x)\n",
    "            x = self.relu(x)\n",
    "            \n",
    "            x = self.unpool(x,indices1,output_size=(26,26))\n",
    "            x = self.unconv2(x)\n",
    "            x = self.tanh(x)\n",
    "\n",
    "        \n",
    "            return x\n",
    "\n",
    "\n",
    "    ''' We will not do any parameter optimization for this tutorial, so no need to have any\n",
    "    parameters for this method'''\n",
    "    def train_test(self):\n",
    "            \n",
    "           \n",
    "            batches = batchify_autoencoder(self.data,batch_size=16)\n",
    "  \n",
    "            neural_network = BIOF510_AC.AutoCNN()\n",
    "        \n",
    "            optimizer = optim.SGD(neural_network.parameters(), lr=0.01)\n",
    "        \n",
    "            loss_function = nn.MSELoss()\n",
    "        \n",
    "            neural_network.train()\n",
    "        \n",
    "            ### n_epochs\n",
    "            for i in range(3):\n",
    "                error = 0\n",
    "                for ii in range(len(batches)):\n",
    "                \n",
    "                    optimizer.zero_grad()\n",
    "                \n",
    "                    batch = batches[ii]\n",
    "\n",
    "                    predictions = neural_network(torch.tensor(np.asarray(batch).astype(np.float32)))\n",
    "                    \n",
    "                    loss = loss_function(predictions,torch.tensor(np.asarray(batch).astype(np.float32)))\n",
    "                \n",
    "                    loss.backward()\n",
    "                \n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    error += loss.data\n",
    "                    \n",
    "                print('Error: ' + str((error/len(self.data))*16))\n",
    "\n",
    "            return neural_network\n",
    "        \n",
    "   \n",
    "\n",
    "\n",
    "''' Utility Function - function to turn the data into batches'''\n",
    "\n",
    "def batchify_autoencoder(data,batch_size=16):\n",
    "    \n",
    "    batches= []\n",
    "\n",
    "\n",
    "    for n in range(0,len(data),batch_size):\n",
    "        if n+batch_size < len(data):\n",
    "            batches.append(data[n:n+batch_size])\n",
    "            \n",
    "\n",
    "    if len(data)%batch_size > 0:\n",
    "        batches.append(data[len(data)-(len(data)%batch_size):len(data)])\n",
    "\n",
    "        \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31.0%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "98.2%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112.7%\n",
      "/Users/alechay/opt/anaconda3/envs/biof510/lib/python3.9/site-packages/torchvision/datasets/mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1616554815452/work/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/MNIST/raw\n",
      "\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "data = torchvision.datasets.MNIST(\n",
    "    root = './data/MNIST',\n",
    "    download = True)\n",
    "\n",
    "\n",
    "labels = data.targets\n",
    "data = data.data\n",
    "newdata = []\n",
    "\n",
    "for image in data:\n",
    "   image = np.ravel(image).astype(np.float64)\n",
    "   image *= 1/image.max()\n",
    "   newdata.append(image.reshape(1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: tensor(0.0443)\n",
      "Error: tensor(0.0167)\n",
      "Error: tensor(0.0126)\n"
     ]
    }
   ],
   "source": [
    "testclass = BIOF510_AC(newdata)\n",
    "model = testclass.train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (biof510)",
   "language": "python",
   "name": "biof510"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
